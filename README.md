# ANN_MNIST
## Objetivo - Descri√ß√£o em Portugu√™s üáßüá∑
O objetivo principal deste projeto √© proporcionar uma compreens√£o aprofundada do funcionamento de redes neurais artificiais, explorando especialmente a influ√™ncia das taxas de aprendizado no desempenho do modelo. Al√©m disso, o projeto visa fornecer insights sobre todo o ciclo de vida da rede neural, desde sua cria√ß√£o at√© o p√≥s-processamento dos resultados.

## Vis√£o Geral
### Arquitetura da Rede Neural

A rede neural √© composta por tr√™s camadas lineares.
A primeira camada (linear1) tem entrada de tamanho 28x28 e 128 neur√¥nios.
A segunda camada (linear2) tem 128 neur√¥nios de entrada e 64 neur√¥nios de sa√≠da.
A terceira camada (linear3) possui 64 neur√¥nios de entrada e 10 neur√¥nios de sa√≠da, correspondendo aos poss√≠veis d√≠gitos de 0 a 9.

### Treinamento
Foi utilizado o otimizador SGD (Gradiente Descendente Estoc√°stico) com uma taxa de aprendizado inicial de 0.01 e momentum de 0.5.
A fun√ß√£o de perda utiliziada foi a NLL (Negative Log Likelihood). Esta fun√ß√£o que itera sobre o conjunto de dados de treinamento, ajusta os pesos da rede e monitora o desempenho por meio da perda acumulada.
A perda monitora √© relativa tanto ao conjunto de treino quanto ao conjunto de avalia√ß√£o. Desta forma, √© poss√≠vel observar se o n√∫mero de √©pocas, infraestrutura da rede e outros detalhes est√£o coerentes e funcionais durante o treinamento.

### Avalia√ß√£o
Algumas m√©tricas foram consideradas para avaliar o modelo. 
- Primeiramente foi medida a acur√°cia do modelo;
- Em seguida, a curva ROC e o valor AUC para cada classe prevista;
- Por fim, a matriz de confus√£o observada.

### Melhorias Cont√≠nuas
Poss√≠veis melhorias poderiam envolver, por exemplo, aumento de dados de forma a possbilitar o modelo a aprender n√∫meros em √¢ngulos diferentes.
Al√©m de tamb√©m explorar diferentes hiperpar√¢metros, t√©cnicas de regulariza√ß√£o e estrat√©gias de treinamento para otimizar o desempenho da rede neural.

### Execu√ß√£o do Projeto
Para executar este projeto, basta instalar as bibliotecas presentes no arquivo 'requirements.txt' ou rodar o arquivo .ipynb no Goggle Colab.

### Contribui√ß√µes
Contribui√ß√µes s√£o bem-vindas! Se voc√™ identificar melhorias, problemas ou tiver sugest√µes, sinta-se √† vontade para contribuir para este projeto.

## Objective - Description in English üá∫üá∏
The main objective of this project is to provide an in-depth understanding of the functioning of artificial neural networks, especially exploring the influence of learning rates on model performance. Furthermore, the project aims to provide insights into the entire neural network lifecycle, from its creation to post-processing of results.

## Overview
### Neural Network Architecture

The neural network is made up of three linear layers.
The first layer (linear1) has input size 28x28 and 128 neurons.
The second layer (linear2) has 128 input neurons and 64 output neurons.
The third layer (linear3) has 64 input neurons and 10 output neurons, corresponding to the possible digits from 0 to 9.


### Training
The SGD (Stochastic Gradient Descent) optimizer was used with an initial learning rate of 0.01 and momentum of 0.5.
The loss function used was NLL (Negative Log Likelihood). This function that iterates over the training dataset, adjusts the network weights, and monitors performance through accumulated loss.
The monitor loss is relative to both the training set and the evaluation set. This way, it is possible to observe whether the number of epochs, network infrastructure and other details are coherent and functional during training.

### Assessment
Some metrics were considered to evaluate the model.
- Firstly, the accuracy of the model was measured;
- Then the ROC curve and the AUC value for each predicted class;
- Finally, the observed confusion matrix.

### Continuous Improvements
Possible improvements could involve, for example, data augmentation in order to enable the model to learn numbers at different angles.
In addition to also exploring different hyperparameters, regularization techniques and training strategies to optimize the performance of the neural network.

### Project Execution
To run this project, simply install the libraries present in the 'requirements.txt' file or run the .ipynb file in Goggle Colab.

### Contributions
Contributions are welcome! If you identify improvements, problems or have suggestions, please feel free to contribute to this project.
